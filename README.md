# [SwarmKit](https://github.com/docker/swarmkit)
[manager/scheduler/rolescheduler.go](https://github.com/foxxxyben/swarmkit/blob/master/manager/scheduler/rolescheduler.go)

This is my personal dev fork of SwarmKit. My current project is to provide Docker with the means to automatically update Node roles in order to maintain a user-defined replica set of Manager nodes. Docker and other container orchestrators do a great job of maintaining sets of services healthy, but if a Manager node fails then human intervention is required to maintain the health of the entire cluster. I had initially tried accomplishing such a setup with unmodified Raft-based orchestrators, namely Nomad via its command line runtime, but this was impossible due to circular dependencies of requiring an initial bootstrap set of Managers whose Scheduler routines were not self-aware of their own state as part of a Service set.

My current solution combines the routines of Scheduler, Orchestrator, and Dispatcher into a single file [rolescheduler.go](https://github.com/foxxxyben/swarmkit/blob/master/manager/scheduler/rolescheduler.go) in the [scheduler](github.com/docker/swarmkit/manager/scheduler) package, utilizing the same scheduling logic used to preferentially assign Tasks to Nodes to now propose Node Role changes, which are fulfilled by existing, unmodified Manager routines via role_manager.go, and responding to NodeStatus Events from dispatcher.go to monitor the health of the Manager set. Combining multiple routines into a single package namespace was necessary because roleScheduler uses existing Service definitions but does *not* use Tasks because the relationship between Node and Role is more tightly coupled than Tasks and while there was a simple, existing way to filter Services based on Mode, there was no such distinction in the Task api definition. This approach is also much more in line with the stated main benefits of *SwarmKit*:

-   **Distributed**: *roleScheduler* runs in and relies on stock subroutines in the Leader Manager loop, and inherits all the distributed benefits of the underlying Raft implementation.
-   **Secure**: *roleScheduler* does not directly modify Node.Role state; it sets DesiredRole based on user-defined Placement Preferences and relies on the existing *SwarmKit* Node communication and membership within an existing Cluster. roleScheduler uses *SwarmKit's* mutual TLS for node *authentication*, *role authorization* and *transport encryption*, certificate issuance and rotation, etc. without modifications.
-   **Simple**: *roleScheduler* adds an additional *ServiceSpec_Manager* mode to api.ServiceSpec.Mode, allowing users to define a Manager set that is identical to *ServiceSpec_Replicated*. Simply ignore the container-specific Task definitions, set any Placement Constraints and Preferences, and a desired number of Replicas, and roleScheduler will maintain a healthy set of Managers as if it were any other Service.
-   **Resource Awareness**: *roleScheduler* inherits a referenced NodeSet from its parent Scheduler and is aware of running Tasks and resources available on nodes and will try to propose Managers accordingly. The choice was made, due to security and integration concerns, for this initial pull request, *not* to drain nodes in order to force resource allocation to new Managers, however in the future this should be made a user-defined option, but will require more changes to the API Service definitions.
-   **Update Strategy**: *roleScheduler* prioritizes quorum health when proposing replacement Manager Nodes:
    -   In normal conditions, where the number of Manager failures does not threaten quorum loss or cluster inconsistency, new Managers are proposed to fill the need, those nodes are marked Pending to prevent further proposals, and if they are not reconciled within a set timeout the Pending lock is removed to allow new proposals, and all Pending and Failed NodeInfo flags are removed once the set number of replicas is achieved. The result is that the first-available nodes that are either newly proposed or recovered from failures are recognized as the Active set of Managers.
    -   If the number of Manager failures is greater than the number of nodes required to maintain a quorum, a timeout is set to block new Manager proposals in order to allow failed Managers a chance to recover first. These timeouts are currently only available as defaults, however they should be user-defined in the API Service definition in the future in order to tune for reasonable boot times; making such boot time allowances part of the Node Resource description might be an option for tailoring these settings to different nodes or types within heterogeneous clusters.
    -   Periodically, *roleScheduler* will propose a single new Manager regardless of the fullness of the replica set; this forces the reconciliation loop to demote the least preferential or performant node from the Manager group, gradually ratcheting up the set to hold higher-ranked nodes. This attempts to avoid and recover from scenarios where high-value nodes fail, recover, but are otherwise blocked for promotion by low-value "emergency" nodes. Again, the timeout should be made user-defined to make sure this "upgrade" rate does not cause Raft replication inconsistency.
    
Signed-off-by: Ben Fox <info@benfox.net>
